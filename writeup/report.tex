\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  
\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{hyperref} %For hyperlinks
\usepackage{float} % To place graphs
\usepackage{epstopdf}
\usepackage{mathrsfs}


\title{\LARGE \bf
Recommendation Systems for Amazon.com
}
\author{Nikhil Johri, Zahan Malkani, and Ying Wang
}
\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Modern retailers frequently use recommendation systems to suggest products of 
interest to a collection of consumers. A closely related task is ratings 
prediction, in which the system predicts a numerical rating that a 
user $u$ will assign to a product $p$. In this paper, we build three ratings 
prediction models for a dataset of products and users from Amazon.com. We 
evaluate the strengths and weaknesses of each model, and discuss their 
effectiveness in a recommendation system.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In this paper, we focus on collaborative filtering methods for recommendations. 
Collaborative filtering is the term applied to techniques that analyze the 
relationships between users and products in a large dataset and make 
recommendations based on similarity between nodes. Similarity depends only on
history -- for example, two users may be similar if they have purchased many 
of the same items, so one user's rating can be used to infer a rating for 
another. The alternative to collaborative filtering is content filtering, which 
creates features for users and products to assess compatibility. In contrast, 
collaborative filtering does not need to create features like genre or subject, 
so it is domain-independent [cite]. 

\subsection{Previous Work -- needs more elaboration}
Collaborative filtering has enjoyed a long popularity in recommendations tasks. 
It was first used commercially in 1992 in a system called Tapestry to 
recommend newsgroup messages to readers [Cite]. Neighborhood-based methods, 
which we explore in two of our models, have been incorporated by Amazon into 
its own recommendation engine [Cite]. Matrix factorization, another model we 
explore, has also been highly successful. [Cite] Modern recommendation systems 
often use a combination of collaborative filtering, content-based filtering, 
and matrix factorization, often using multiple methods from each category. 
The top entrants in the Netflix Prize used such hybrid approaches, employing 
as many as 100 different techniques. [Cite]

\subsection{Our project}
This project explores some of the most popular ratings prediction methods using 
a dataset from Amazon.com. The dataset, described in Section 
\ref{sec:dataset}, contains product purchase metadata of over 500,000 DVDs, 
music albums, books, and videos. We use both neighborhood-based and 
matrix factorization methods, described in Section \ref{sec:models}, and we 
discuss our findings in in Section \ref{sec:results}. Based on our 
experiments, we hope to shed some light on the nature of the recommendation 
task and the strengths of each of the models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset}
\label{sec:dataset}
\subsection{Preliminary Yelp dataset}

\subsection{Amazon dataset}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Models}
\label{sec:models}

\subsection{Neighborhood-based model}
\subsection{Modified neighborhood model}
\subsection{Item-based model}
\subsection{Matrix factorization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}
\label{sec:results}

We report the results of our models on the full Amazon dataset and on our 
high-activity subset. We measure performance over the test sets using 
root-mean-square error. The errors are normalized by dividing by 4, the maximum 
difference between the highest and lowest star ratings possible. However, 
in the interest of preserving granularity for model comparison, predicted
fractional ratings are not rounded to the nearest integer before calculating 
the errors. 

\subsection{Neighborhood-based model}
\subsubsection{Full dataset}
The results of the neighborhood-based model on the full dataset are shown in 
table ?? and Figure ?? below.

%% \begin{figure}[htp]
%% \includegraphics[scale=0.5]{pert.eps}
%% \caption{Effect of perturbation $(\epsilon)$ on the Average Error}
%% \label{fig:pert}
%% \end{figure}

\begin{table}[htb]
\centering
\begin{tabular}{|c|c|c|}
\cline{2-3}

\multicolumn{1}{c|}{} & \vbox{\hbox{\strut Neighborhood model}} 
& \vbox{\hbox{\strut Modified }\hbox{\strut neighborhood model}} \tabularnewline \hline
$k$ = 1 & 0.3497 & 0.3870 \tabularnewline
$k$ = 3 &  0.3374 & 0.3544 \tabularnewline
$k$ = 5 & 0.3348 & 0.3448 \tabularnewline
$k$ = 10 & 0.3262 & 0.3397 \tabularnewline
$k$ = 25  & 0.3220 & 0.3346 \tabularnewline
\hline
All other users & \multicolumn{2}{|c|}{?}  \tabularnewline
\hline
Always predict 4 & \multicolumn{2}{|c|}{0.3211}  \tabularnewline
\hline
\end{tabular}
\caption{Neighborhood models, full dataset}
\end{table}

\subsubsection{High-activity dataset}

\begin{table}[htb]
\centering
\begin{tabular}{|c|c|c|}
\cline{2-3}

\multicolumn{1}{c|}{} & \vbox{\hbox{\strut Neighborhood model}} 
& \vbox{\hbox{\strut Modified }\hbox{\strut neighborhood model}} \tabularnewline \hline
$k$ = 1 &  & 0.1397 \tabularnewline
$k$ = 3 &  & 0.1864 \tabularnewline
$k$ = 5 &  & 0.2180 \tabularnewline
$k$ = 10 & & 0.2493 \tabularnewline
$k$ = 25  &  & 0.2645 \tabularnewline
\hline
All other users & \multicolumn{2}{|c|}{}  \tabularnewline
\hline
Always predict 4 & \multicolumn{2}{|c|}{0.3039}  \tabularnewline
\hline
\end{tabular}
\caption{Neighborhood models, full dataset}
\end{table}


\subsection{Item-based model}
\subsection{Matrix factorization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}



%% \begin{thebibliography}{99}
%% \bibitem{tutorial}
%% Smola, A. J., and Scholkopf, B., (1998), ``A Tutorial on Support Vector Regression,'' NeuroCOLT2 Technical Report Series.

%% \bibitem{joac}
%% Joachims, T., (2002), ``Optimizing Search Engines using Clickthrough Data,'' ACM Conference on Knowledge Discovery and Data Mining (KDD).

%% \bibitem{coord1}
x%% Chang, K.W., Cho, J.H, and Chih, J.L., (2008), ``Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines,'' Journal of Machine Learning Research 9 (2008),  pp.1369-1398.

%% \bibitem{coord2}
%% Chang, K.W., Cho, J.H, Chih, J.L., Sathiya Keerthi, S., and Sundararajan, S. (2008), ``A Dual Coordinate Descent Method for Large-scale Linear SVM,'' 25$^{th}$ International Conference on Machine Learning, Helsinki, Finland.


%% \end{thebibliography}

\end{document}
