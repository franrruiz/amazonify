\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  
\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{hyperref} %For hyperlinks
\usepackage{float} % To place graphs
\usepackage{epstopdf}
\usepackage{mathrsfs}


\title{\LARGE \bf
Recommendation Systems for Amazon.com
}
\author{Nikhil Johri, Zahan Malkani, and Ying Wang
}
\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Modern retailers frequently use recommendation systems to suggest products of 
interest to a collection of consumers. A closely related task is ratings 
prediction, in which the system predicts a numerical rating that a 
user $u$ will assign to a product $p$. In this paper, we build three ratings 
prediction models for a dataset of products and users from Amazon.com. We 
evaluate the strengths and weaknesses of each model, and discuss their 
effectiveness in a recommendation system.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In this paper, we focus on collaborative filtering methods for recommendations. 
Collaborative filtering is the term applied to techniques that analyze the 
relationships between users and products in a large dataset and make 
recommendations based on existing connections between nodes \cite{bib:recsys}.
One common technique in collaborative filtering is to use existing connections 
to make judgments about similar products and users. Similarity depends only on 
history -- for example, two users may be similar if they have purchased many 
of the same items, so one user's rating can be used to infer a rating for 
another. 

The alternative to collaborative filtering is content filtering, which 
creates features for users and products to assess compatibility. These 
features, which in a book recommendation system might be things like genre or 
subject, will be scored for both users and products \cite{bib:recsys}. This 
makes content filtering highly domain-specific. 
In contrast, collaborative filtering does not need to create such features, 
so it is domain-independent. It is sufficient in collorative filtering to have 
only a matrix of users to products, where each entry in the matrix is some 
scalar indicator of the past relationship between a user and a product.

\subsection{Previous Work}
Collaborative filtering has enjoyed a long popularity in recommendations tasks. 
It was first used commercially in 1992 in a system called Tapestry to 
recommend newsgroup messages to readers \cite{bib:tapestry}. In this system, 
feedback and annotations from existing user-document relationships are used 
to select interesting documents for other users. This system first uses the 
term \emph{collaborative filtering} to indicate that people implicitly 
collaborate by recording their reactions to documents, enabling 
others to make decisions based on those reactions.

Our work is based on two broad categories of collaborative filtering: 
similarity methods and matrix factorization \cite{bib:recsys2}. Similarity 
methods make recommendations by comparing the similarity between users or 
products. In a neighborhood-based similarity model, users are compared to each 
other to determine their nearest neighbors based on their histories. Then, to 
make a prediction for user $u$'s opinion on product $p$, the model looks at the 
opinions of the neighbors of $u$ regarding $p$. Another similarity model is the 
item-based model, which examines item similarity instead of user similarity. 
This approach has been the basis for Amazon's own recommendation engine
 \cite{bib:amazon}. Its advantage is that product similarities can be computed 
offline, and when a user needs a product recommendation, the system performs 
a fast lookup of items similar to ones in the user's history . This speed 
has been beneficial for scalability in Amazon's large purchase network. 

The second type of collaborative filtering is model-based methods, in our case, 
matrix factorization. Matrix factorization does not use history to  model 
similarity like the previously discussed models. Instead, it uses past ratings 
to estimate the parameters of a statistical model for user-product 
relationships \cite{bib:matrixfact}. Users and products are represented as 
vectors in a latent vector space $R^f$. A numerical estimate for the 
opinion of  user $u$ on product $p$ can be obtained by taking the 
cross product of vectors for $u$ and $p$. The values of the latent dimensions 
are learned during a training stage by minimizing the error between known and 
predicted ratings.

Modern recommendation systems are often a combination of 
collaborative filtering, content-based filtering, and matrix factorization. 
One way to create a hybrid model is simply to take the outcomes of several 
approaches and merge them by taking a weighted average. Other 
hybrid techniques incorporate the previously discussed models with other
machine learning methods, such as classification \cite{bib:recsys2}. 
The ways to combine the approaches are numerous, and it is common for 
a recommendation system to incorporate a large number of strategies. 
The top entrants in the Netflix Prize used a hybrid algorithm with over 100 
techniques \cite{bib:bellkor}.

\subsection{Our project}
This project explores some of the most popular ratings prediction methods using 
a dataset from Amazon.com. The dataset, described in Section 
\ref{sec:dataset}, contains product purchase metadata of over 500,000 DVDs, 
music albums, books, and videos. We use both neighborhood-based and 
matrix factorization methods, described in Section \ref{sec:models}, and we 
discuss our findings in in Section \ref{sec:results}. Based on our 
experiments, we hope to shed some light on the nature of the recommendation 
task and the strengths of each of the models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset}
\label{sec:dataset}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Models}
\label{sec:models}

\subsection{Neighborhood-based model}
\subsection{Modified neighborhood model}
\subsection{Item-based model}
\subsection{Matrix factorization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}
\label{sec:results}

We report the results of our models on the full Amazon dataset and on our 
high-activity subset. We measure performance over the test sets using 
root-mean-square error. The errors are normalized by dividing by 4, the maximum 
difference between the highest and lowest star ratings possible. However, 
in the interest of preserving granularity for model comparison, predicted
fractional ratings are not rounded to the nearest integer before calculating 
the errors. 

\subsection{Neighborhood-based model}
\subsubsection{Full dataset}
The results of the neighborhood-based model on the full dataset are shown in 
table ?? and Figure ?? below.

%% \begin{figure}[htp]
%% \includegraphics[scale=0.5]{pert.eps}
%% \caption{Effect of perturbation $(\epsilon)$ on the Average Error}
%% \label{fig:pert}
%% \end{figure}

\begin{table}[htb]
\centering
\begin{tabular}{|c|c|c|}
\cline{2-3}

\multicolumn{1}{c|}{} & \vbox{\hbox{\strut Neighborhood model}} 
& \vbox{\hbox{\strut Modified }\hbox{\strut neighborhood model}} \tabularnewline \hline
$k$ = 1 & 0.3497 & 0.3870 \tabularnewline
$k$ = 3 &  0.3374 & 0.3544 \tabularnewline
$k$ = 5 & 0.3348 & 0.3448 \tabularnewline
$k$ = 10 & 0.3262 & 0.3397 \tabularnewline
$k$ = 25  & 0.3220 & 0.3346 \tabularnewline
\hline
All other users & \multicolumn{2}{|c|}{?}  \tabularnewline
\hline
Always predict 4 & \multicolumn{2}{|c|}{0.3211}  \tabularnewline
\hline
\end{tabular}
\caption{Neighborhood models, full dataset}
\end{table}

\subsubsection{High-activity dataset}

\begin{table}[htb]
\centering
\begin{tabular}{|c|c|c|}
\cline{2-3}

\multicolumn{1}{c|}{} & \vbox{\hbox{\strut Neighborhood model}} 
& \vbox{\hbox{\strut Modified }\hbox{\strut neighborhood model}} \tabularnewline \hline
$k$ = 1 &  & 0.1397 \tabularnewline
$k$ = 3 &  & 0.1864 \tabularnewline
$k$ = 5 &  & 0.2180 \tabularnewline
$k$ = 10 & & 0.2493 \tabularnewline
$k$ = 25  &  & 0.2645 \tabularnewline
\hline
All other users & \multicolumn{2}{|c|}{}  \tabularnewline
\hline
Always predict 4 & \multicolumn{2}{|c|}{0.3039}  \tabularnewline
\hline
\end{tabular}
\caption{Neighborhood models, full dataset}
\end{table}


\subsection{Item-based model}
\subsection{Matrix factorization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}


\begin{thebibliography}{99}

\bibitem{bib:recsys}
Adomavicius, G., Tuzhilin, A. (2005). Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions. Knowledge and Data Engineering, IEEE Transactions on , 17(6),  734- 749.

\bibitem{bib:matrixfact}
Bell, R., Koren, Y., Volinsky, C. (2009). Matrix factorization techniques for recommender systems. IEEE Computer 42(8):30-37

\bibitem{bib:bellkor}
Bell, R., Koren, Y., Volinsky, C. (2009). 
The BellKor solution to the Netï¬‚ix Prize. Technical Report, AT\&T Labs 
Research, 2007b. 
http://www.netflixprize.com/assets/ProgressPrize2007\_KorBell.pdf


\bibitem{bib:tapestry}
Goldberg, D., Nichols, D., Oki, B., Terry, D. (1992). 
Using collaborative filtering to weave an information tapestry. 
Communications of the Association of Computing Machinery, 35(12), 61-70.

\bibitem{bib:amazon}
Linden, G., Smith, B., York, J. (2003). Amazon.com recommendations: Item-to-item collaborative filtering. IEEE Internet
Computing, 7(1), 76-80.

\bibitem{bib:recsys2}
Melville, P., Sindhwani, V. (2010).
Recommender Systems. The Encyclopedia of Machine Learning. 
http://www.prem-melville.com/publications/recommender-systems-eml2010.pdf

\end{thebibliography}

\end{document}
